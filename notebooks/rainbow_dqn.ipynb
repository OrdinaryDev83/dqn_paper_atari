{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Atari Paper Upgrade using Rainbow\n",
    "Here we will upgrade the DQN Atari paper using the Rainbow algorithm.\n",
    "From the collection of improvements in the Rainbow algorithm, we will implement the following:\n",
    "- Dueling Network Architecture\n",
    "- Prioritized Experience Replay\n",
    "- N-Step Returns\n",
    "- Noisy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13b457390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 150_000\n",
    "MINI_BATCH_SIZE = 32\n",
    "TARGET_UPDATE_FREQ = 1_200\n",
    "FRAME_SKIP = 4\n",
    "MIN_EPSILON = 0.1\n",
    "MAX_EPSILON = 1.0\n",
    "EPSILON_PHASE = 0.1\n",
    "MAX_STEPS = 1_500_001\n",
    "REPLAY_START_SIZE = 75_000\n",
    "SAVE_FREQUENCY = 500_000\n",
    "\n",
    "N_STEP = 3  # For N-Step Returns\n",
    "ALPHA = 0.6  # Prioritization exponent\n",
    "BETA_START = 0.4  # Initial value of beta for importance sampling\n",
    "BETA_FRAMES = MAX_STEPS - REPLAY_START_SIZE  # Schedule for beta\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Replay Buffer\n",
    "The paper introduces Prioritized Replay Buffer to sample important transitions more frequently. We use a SumTree data structure to store priorities and sample transitions based on the priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha, obs_shape):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.pos = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-allocate memory for buffer components\n",
    "        self.states = np.empty((capacity, *obs_shape), dtype=np.uint8)\n",
    "        self.next_states = np.empty((capacity, *obs_shape), dtype=np.uint8)\n",
    "        self.actions = np.empty((capacity,), dtype=np.int32)\n",
    "        self.rewards = np.empty((capacity,), dtype=np.float32)\n",
    "        self.dones = np.empty((capacity,), dtype=np.bool_)\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, error, state, next_state, action, reward, done):\n",
    "        self.states[self.pos] = state\n",
    "        self.next_states[self.pos] = next_state\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.dones[self.pos] = done\n",
    "\n",
    "        self.priorities[self.pos] = self.max_priority ** self.alpha\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        if self.size == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.size]\n",
    "\n",
    "        probs = prios / prios.sum()\n",
    "        indices = np.random.choice(self.size, batch_size, p=probs)\n",
    "\n",
    "        # Retrieve samples directly without using zip()\n",
    "        states = self.states[indices]\n",
    "        next_states = self.next_states[indices]\n",
    "        actions = self.actions[indices]\n",
    "        rewards = self.rewards[indices]\n",
    "        dones = self.dones[indices]\n",
    "\n",
    "        total = self.size\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = weights.astype(np.float32)\n",
    "\n",
    "        return states, next_states, actions, rewards, dones, indices, weights\n",
    "\n",
    "    def update(self, idxs, errors):\n",
    "        errors = np.abs(errors) + 1e-6\n",
    "        self.max_priority = max(self.max_priority, errors.max())\n",
    "        self.priorities[idxs] = errors ** self.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Linear Layers\n",
    "The paper introduces Noisy Linear Layers to add noise to the weights of the linear layers. We use a NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.FloatTensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.weight_mu.size(1))\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / np.sqrt(self.weight_sigma.size(1))\n",
    "        )\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features).to(device)\n",
    "        epsilon_out = self._scale_noise(self.out_features).to(device)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = (\n",
    "                self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            )\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign() * x.abs().sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture\n",
    "Dueling Network Architecture is used in the Rainbow algorithm. The architecture consists of two streams, one for the state value and the other for the advantage values. The two streams are combined to produce the Q-values.\n",
    "\n",
    "We will also use the previously implemented NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.fc_input_dim = 7 * 7 * 64\n",
    "\n",
    "        # Dueling DQN streams with NoisyLinear layers\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, 1),\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x / 255.0)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_vals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_vals\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "As in the paper, we use an epsilon-greedy policy to select actions during training. We start with a high epsilon value and decay it over time. In addition to the NoisyLinear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the gym environment, by selecting the Breakout game. We specify RMSProp as the optimizer just like in the paper training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, render_mode=None, frame_skip=4):\n",
    "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode, frameskip=1)\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    env = gym.wrappers.AutoResetWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(\"ALE/Breakout-v5\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "dqn = DeepQNetwork(n_actions).to(device)\n",
    "optimizer = torch.optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "dqn_prime = DeepQNetwork(n_actions).to(device)\n",
    "\n",
    "buffer = PrioritizedReplayBuffer(\n",
    "    REPLAY_MEMORY_SIZE, alpha=ALPHA, obs_shape=(4, 84, 84)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 6,507,690\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "print(f\"Trainable parameters: {count_trainable_parameters(dqn):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNetwork(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       "  (advantage_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we follow the methodology from the paper, by putting all the above components together to train the DQN agent on the Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = {\n",
    "    \"loss\": [0],\n",
    "    \"mean_q_value\": [0],\n",
    "    \"episode_rewards\": [0],\n",
    "    \"steps\": [0],\n",
    "}\n",
    "\n",
    "t_observation, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "progress_bar = tqdm(range(MAX_STEPS), desc=\"Training Progress\")\n",
    "\n",
    "episode_steps = 0\n",
    "episode_loss = 0\n",
    "episode_q_values = 0\n",
    "\n",
    "n_step_buffer = []\n",
    "\n",
    "for t in progress_bar:\n",
    "    # Epsilon with linear decay\n",
    "    eps = max(\n",
    "        MIN_EPSILON,\n",
    "        MIN_EPSILON\n",
    "        + (MAX_EPSILON - MIN_EPSILON) * (1 - t / (EPSILON_PHASE * MAX_STEPS)),\n",
    "    )\n",
    "\n",
    "    # Epsilon-greedy policy with Noisy Networks\n",
    "    if np.random.rand() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            dqn.reset_noise()\n",
    "            obs_tensor = torch.tensor(\n",
    "                np.array(t_observation), device=device, dtype=torch.float32\n",
    "            ).unsqueeze(0)\n",
    "            q_values = dqn(obs_tensor)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "            episode_q_values += q_values.mean().item()\n",
    "\n",
    "    # Step environment\n",
    "    t1_observation, reward, done, _, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Add to n-step buffer\n",
    "    n_step_buffer.append((t_observation, action, reward, done, t1_observation))\n",
    "\n",
    "    if len(n_step_buffer) >= N_STEP:\n",
    "        # Compute N-step return\n",
    "        R = sum(\n",
    "            [\n",
    "                n_step_buffer[i][2] * (DISCOUNT_FACTOR ** i)\n",
    "                for i in range(N_STEP)\n",
    "            ]\n",
    "        )\n",
    "        state = n_step_buffer[0][0]\n",
    "        action_n = n_step_buffer[0][1]\n",
    "        next_state = n_step_buffer[-1][4]\n",
    "        done_n = n_step_buffer[-1][3]\n",
    "\n",
    "        buffer.add(\n",
    "            error=buffer.max_priority,\n",
    "            state=state,\n",
    "            next_state=next_state,\n",
    "            action=action_n,\n",
    "            reward=R,\n",
    "            done=done_n,\n",
    "        )\n",
    "\n",
    "        n_step_buffer.pop(0)\n",
    "\n",
    "    if done:\n",
    "        # Add remaining transitions in n-step buffer\n",
    "        while len(n_step_buffer) > 0:\n",
    "            len_buffer = len(n_step_buffer)\n",
    "            R = sum(\n",
    "                [\n",
    "                    n_step_buffer[i][2] * (DISCOUNT_FACTOR ** i)\n",
    "                    for i in range(len_buffer)\n",
    "                ]\n",
    "            )\n",
    "            state = n_step_buffer[0][0]\n",
    "            action_n = n_step_buffer[0][1]\n",
    "            next_state = n_step_buffer[-1][4]\n",
    "            done_n = n_step_buffer[-1][3]\n",
    "\n",
    "            buffer.add(\n",
    "                error=buffer.max_priority,\n",
    "                state=state,\n",
    "                next_state=next_state,\n",
    "                action=action_n,\n",
    "                reward=R,\n",
    "                done=done_n,\n",
    "            )\n",
    "\n",
    "            n_step_buffer.pop(0)\n",
    "\n",
    "        training_history[\"steps\"].append(t)\n",
    "        training_history[\"episode_rewards\"].append(episode_reward)\n",
    "        training_history[\"mean_q_value\"].append(\n",
    "            episode_q_values / episode_steps if episode_steps > 0 else 0\n",
    "        )\n",
    "        training_history[\"loss\"].append(\n",
    "            episode_loss / episode_steps if episode_steps > 0 else 0\n",
    "        )\n",
    "        episode_reward = 0\n",
    "        progress_bar.set_description(\n",
    "            f\"R: {training_history['episode_rewards'][-1]:.2f}, l: {training_history['loss'][-1]:.2f}, Mean Q: {training_history['mean_q_value'][-1]:.2f}, e: {eps:.2f}\"\n",
    "        )\n",
    "        episode_steps = 0\n",
    "        episode_loss = 0\n",
    "        episode_q_values = 0\n",
    "\n",
    "        t_observation, _ = env.reset()\n",
    "    else:\n",
    "        t_observation = t1_observation\n",
    "\n",
    "    # Checkpoint every SAVE_FREQUENCY steps\n",
    "    if t > 0 and t % SAVE_FREQUENCY == 0:\n",
    "        torch.save(dqn.state_dict(), f\"checkpoint{t}.pt\")\n",
    "\n",
    "    if t > REPLAY_START_SIZE:\n",
    "        if t % 4 == 0:\n",
    "            beta = min(\n",
    "                1.0,\n",
    "                BETA_START + (t - REPLAY_START_SIZE) * (1.0 - BETA_START) / BETA_FRAMES,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                states,\n",
    "                next_states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                dones,\n",
    "                idxs,\n",
    "                is_weights,\n",
    "            ) = buffer.sample(MINI_BATCH_SIZE, beta)\n",
    "\n",
    "            # Convert to tensors\n",
    "            states = torch.tensor(states, device=device, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, device=device, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "            rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones.astype(np.float32), device=device)\n",
    "\n",
    "            is_weights = torch.tensor(is_weights, device=device, dtype=torch.float32)\n",
    "\n",
    "            # Reset noise in the networks\n",
    "            dqn.reset_noise()\n",
    "            dqn_prime.reset_noise()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                not_done = 1.0 - dones\n",
    "                # Double DQN with N-Step Returns\n",
    "                next_q_values = dqn_prime(next_states)\n",
    "                next_actions = dqn(next_states).argmax(dim=1)\n",
    "                next_q_values = next_q_values.gather(\n",
    "                    1, next_actions.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                y_j = rewards + (DISCOUNT_FACTOR ** N_STEP) * next_q_values * not_done\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            q_values = dqn(states)\n",
    "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            td_errors = y_j - q_values\n",
    "            loss = (is_weights * td_errors.pow(2)).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            episode_loss += loss.item()\n",
    "\n",
    "            # Update priorities in the buffer\n",
    "            errors = td_errors.detach().cpu().numpy()\n",
    "            buffer.update(idxs, errors)\n",
    "\n",
    "        if t % TARGET_UPDATE_FREQ == 0:\n",
    "            dqn_prime = deepcopy(dqn)\n",
    "\n",
    "    episode_steps += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The trainings were run on Kaggle, so the training logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_plot_infos = pd.DataFrame(training_history)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_plot_infos.to_csv(\"../data/rainbow_dqn_training_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "*The tests were run on Kaggle, so the tests logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path):\n",
    "    \"\"\"\n",
    "    Test the model\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model checkpoint to load\n",
    "    \"\"\"\n",
    "\n",
    "    env = gym.make(\"ALE/Breakout-v5\", frameskip=1, render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    env = gym.wrappers.AutoResetWrapper(env)\n",
    "\n",
    "    # load model\n",
    "    dqn = DeepQNetwork(env.action_space.n).to(device)\n",
    "    state_dict = torch.load(model_path, map_location=torch.device(device))\n",
    "    dqn.load_state_dict(state_dict=state_dict)\n",
    "    dqn.eval()\n",
    "\n",
    "    t_observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if np.random.rand(1) < 0.01:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = dqn(\n",
    "            torch.tensor(np.array(t_observation), device=device).unsqueeze(0)\n",
    "            )\n",
    "            action = torch.argmax(q_values, dim=1).cpu().numpy().squeeze()\n",
    "        t_observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(50):\n",
    "    rewards.append(test_model(f\"../checkpoints/rainbow/checkpoint1500000.pt\"))\n",
    "\n",
    "print(f\"Mean reward: {np.mean(rewards)}\")\n",
    "print(f\"Std reward: {np.std(rewards)}\")\n",
    "print(f\"Max reward: {np.max(rewards)}\")\n",
    "print(f\"Min reward: {np.min(rewards)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
