{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Atari Paper Upgrade using Rainbow\n",
    "Here we will upgrade the DQN Atari paper using the Rainbow algorithm.\n",
    "From the collection of improvements in the Rainbow algorithm, we will implement the following:\n",
    "- Dueling Network Architecture\n",
    "- Prioritized Experience Replay\n",
    "- N-Step Returns\n",
    "- Noisy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13390b3b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00025  # Paper used a similar learning rate\n",
    "DISCOUNT_FACTOR = 0.99  # The Î³ discount factor as mentioned in the paper\n",
    "REPLAY_MEMORY_SIZE = 150_000  # Large replay buffer as described, but not too large\n",
    "BATCH_SIZE = 32  # Minibatch size for training\n",
    "TARGET_UPDATE_FREQ = 1_250  # C steps for target network update\n",
    "FRAME_SKIP = 4  # Number of frames skipped\n",
    "MIN_EPSILON = 0.1  # Minimum value of epsilon (for more exploitation)\n",
    "MAX_EPSILON = 1.0  # Starting value of epsilon (for exploration)\n",
    "EPSILON_PHASE = 0.1  # Percentage of steps for epsilon to reach MIN_EPSILON\n",
    "MAX_STEPS = 100_000  # Total training episodes\n",
    "REPLAY_START_SIZE = 75_000  # Size of replay memory before starting training\n",
    "SAVE_FREQUENCY = 50_000  # Save model every 50k steps\n",
    "\n",
    "# Prioritized Experience Replay parameters\n",
    "ALPHA = 0.6  # Prioritization exponent\n",
    "BETA_START = 0.4  # Initial value of beta for importance sampling weights\n",
    "BETA_FRAMES = MAX_STEPS  # Number of frames over which beta increases to 1\n",
    "\n",
    "# N-step returns\n",
    "N_STEP = 3  # Number of steps for N-step returns\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Replay Buffer\n",
    "The paper introduces Prioritized Replay Buffer to sample important transitions more frequently. We use a SumTree data structure to store priorities and sample transitions based on the priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size, obs_shape, action_shape, alpha):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "\n",
    "        self.t_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
    "        self.t1_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
    "        self.actions = np.empty((size, *action_shape), dtype=np.uint8)\n",
    "        self.rewards = np.empty(size, dtype=np.float16)\n",
    "        self.dones = np.empty(size, dtype=np.bool_)\n",
    "\n",
    "        self.priorities = np.zeros((size,), dtype=np.float32)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "        self.idx = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def append(self, t_obs, t1_obs, actions, reward, done):\n",
    "        self.t_obs[self.idx] = t_obs\n",
    "        self.t1_obs[self.idx] = t1_obs\n",
    "        self.actions[self.idx] = actions\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.dones[self.idx] = done\n",
    "\n",
    "        self.priorities[self.idx] = self.max_priority\n",
    "\n",
    "        self.current_size = min(self.current_size + 1, self.size)\n",
    "        self.idx = (self.idx + 1) % self.size\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        if self.current_size == self.size:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[: self.current_size]\n",
    "        probabilities = priorities**self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        indices = np.random.choice(self.current_size, batch_size, p=probabilities)\n",
    "        weights = (self.current_size * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # Normalize for stability\n",
    "\n",
    "        batch = (\n",
    "            self.t_obs[indices],\n",
    "            self.t1_obs[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.dones[indices],\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            tuple(\n",
    "                torch.as_tensor(item, dtype=torch.float32).to(device) for item in batch\n",
    "            ),\n",
    "            torch.as_tensor(indices).to(device),\n",
    "            torch.as_tensor(weights, dtype=torch.float32).to(device),\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        self.priorities[indices] = priorities\n",
    "        self.max_priority = max(self.max_priority, priorities.max())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Linear Layers\n",
    "The paper introduces Noisy Linear Layers to add noise to the weights of the linear layers. We use a NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Learnable parameters\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.FloatTensor(out_features, in_features)\n",
    "        )\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.FloatTensor(out_features))\n",
    "        self.sigma_init = sigma_init\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize parameters\n",
    "        mu_range = 1 / self.in_features**0.5\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init)\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # Sample new noise\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(input, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture\n",
    "Dueling Network Architecture is used in the Rainbow algorithm. The architecture consists of two streams, one for the state value and the other for the advantage values. The two streams are combined to produce the Q-values.\n",
    "\n",
    "We will also use the previously implemented NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(torch.nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            # Convolution layers (as per paper), input: 84x84x4 image\n",
    "            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            # Using ReLU activations as specified in the paper\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "        # Input features after flattening\n",
    "        conv_out_size = 7 * 7 * 64\n",
    "\n",
    "        # Value stream\n",
    "        self.value_stream = torch.nn.Sequential(\n",
    "            NoisyLinear(conv_out_size, 512), torch.nn.ReLU(), NoisyLinear(512, 1)\n",
    "        )\n",
    "\n",
    "        # Advantage stream\n",
    "        self.advantage_stream = torch.nn.Sequential(\n",
    "            NoisyLinear(conv_out_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            NoisyLinear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x / 255.0)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # Reset noise in NoisyLinear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "As in the paper, we use an epsilon-greedy policy to select actions during training. We start with a high epsilon value and decay it over time. In addition to the NoisyLinear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, epsilon, env):\n",
    "    if np.random.rand(1) < epsilon:  # Explore\n",
    "        return env.action_space.sample()\n",
    "    else:  # Exploit\n",
    "        with torch.no_grad():\n",
    "            policy_net.reset_noise()\n",
    "            state = (\n",
    "                torch.tensor(np.array(state), dtype=torch.float32)\n",
    "                .to(device)\n",
    "                .unsqueeze(0)\n",
    "            )\n",
    "            return policy_net(state).argmax(dim=1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the gym environment, by selecting the Breakout game. We specify RMSProp as the optimizer just like in the paper training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, render_mode=None, frame_skip=4):\n",
    "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode, frameskip=1)\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(\"ALE/Breakout-v5\")\n",
    "action_space = [i for i in range(env.action_space.n)]\n",
    "\n",
    "# Initialize Dueling Noisy Networks\n",
    "policy_net = DuelingDQN(env.action_space.n).to(device)\n",
    "target_net = DuelingDQN(env.action_space.n).to(device)\n",
    "\n",
    "# Use Adam instead to speed up training\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = PrioritizedReplayBuffer(\n",
    "    REPLAY_MEMORY_SIZE, (4, 84, 84), (1,), alpha=ALPHA\n",
    ")\n",
    "\n",
    "epsilon = MAX_EPSILON  # Starting value of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = [\"Image Sequence\"]\n",
    "output_names = [\"Q-Values\"]\n",
    "torch.onnx.export(\n",
    "    policy_net.to(\"cpu\"),\n",
    "    torch.rand((1, 4, 84, 84)),\n",
    "    \"rainbow.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 6,507,690\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "print(f\"Trainable parameters: {count_trainable_parameters(policy_net):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DuelingDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       "  (advantage_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we follow the methodology from the paper, by putting all the above components together to train the DQN agent on the Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R: 2.0, Îµ: 0.10000, RSize: 100442 Q-value: 0.08875, Loss: 0.00020: : 100101it [05:53, 283.34it/s]                         \n"
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "\n",
    "plot_infos = {\n",
    "    \"total_steps\": [],\n",
    "    \"total_reward\": [],\n",
    "    \"epsilon\": [],\n",
    "    \"total_q_values\": [],\n",
    "    \"total_loss\": [],\n",
    "}\n",
    "\n",
    "progress_bar = tqdm(total=MAX_STEPS, desc=\"Training Progress\")\n",
    "\n",
    "n_step_buffer = deque(maxlen=N_STEP)\n",
    "gamma = DISCOUNT_FACTOR\n",
    "\n",
    "while total_steps < MAX_STEPS:\n",
    "    state, info = env.reset()  # Reset environment to initial state\n",
    "    n_step_buffer.clear()\n",
    "\n",
    "    # Decay epsilon for exploration-exploitation tradeoff\n",
    "    epsilon = max(\n",
    "        MIN_EPSILON,\n",
    "        MAX_EPSILON\n",
    "        - (total_steps * (MAX_EPSILON - MIN_EPSILON) / (MAX_STEPS * EPSILON_PHASE)),\n",
    "    )\n",
    "\n",
    "    total_reward = 0\n",
    "    total_q_values = 0\n",
    "    total_loss = 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, policy_net, epsilon, env)\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Clip the reward to be in the range [-1, 1] as mentioned in the paper\n",
    "        total_reward += reward\n",
    "        reward = np.sign(reward)\n",
    "\n",
    "        # Store the transition in N-step buffer\n",
    "        n_step_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(n_step_buffer) == N_STEP:\n",
    "            # Get N-step transition and add to replay buffer\n",
    "            n_state, n_action, n_reward, n_next_state, n_done = (\n",
    "                n_step_buffer[0][0],\n",
    "                n_step_buffer[0][1],\n",
    "                0,\n",
    "                next_state,\n",
    "                done,\n",
    "            )\n",
    "            for idx, (s, a, r, s_, d) in enumerate(n_step_buffer):\n",
    "                n_reward += (gamma**idx) * n_step_buffer[idx][2]\n",
    "                if n_step_buffer[idx][4]:\n",
    "                    n_done = True\n",
    "                    n_next_state = n_step_buffer[idx][3]\n",
    "                    break\n",
    "            replay_buffer.append(\n",
    "                n_state, n_next_state, np.array([n_action]), n_reward, n_done\n",
    "            )\n",
    "\n",
    "        total_steps += 1\n",
    "        episode_steps += 1\n",
    "\n",
    "        # Only start training when replay memory has enough samples\n",
    "        if len(replay_buffer) >= REPLAY_START_SIZE:\n",
    "            if total_steps % 4 == 0:  # Update every 4 steps like in the paper\n",
    "                beta = min(\n",
    "                    1.0, BETA_START + total_steps * (1.0 - BETA_START) / BETA_FRAMES\n",
    "                )\n",
    "\n",
    "                # Sample minibatch from replay buffer\n",
    "                (\n",
    "                    (\n",
    "                        batch_state,\n",
    "                        batch_next_state,\n",
    "                        batch_action,\n",
    "                        batch_reward,\n",
    "                        batch_done,\n",
    "                    ),\n",
    "                    indices,\n",
    "                    weights,\n",
    "                ) = replay_buffer.sample(BATCH_SIZE, beta)\n",
    "\n",
    "                # Compute Q targets using Double DQN\n",
    "                with torch.no_grad():\n",
    "                    not_done = ~batch_done.bool()\n",
    "                    policy_net.reset_noise()\n",
    "                    target_net.reset_noise()\n",
    "                    # Double DQN target computation\n",
    "                    next_q_actions = policy_net(batch_next_state).argmax(\n",
    "                        dim=1, keepdim=True\n",
    "                    )\n",
    "                    next_q_values = (\n",
    "                        target_net(batch_next_state)\n",
    "                        .gather(1, next_q_actions)\n",
    "                        .squeeze(1)\n",
    "                    )\n",
    "                    target_q_values = (\n",
    "                        batch_reward\n",
    "                        + (DISCOUNT_FACTOR**N_STEP) * next_q_values * not_done\n",
    "                    )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Get current Q estimates\n",
    "                policy_net.reset_noise()\n",
    "                q_values = policy_net(batch_state)\n",
    "                batch_action = batch_action.long()\n",
    "                idx = torch.arange(batch_action.size(0)).to(device).long()\n",
    "                values = q_values[idx, batch_action.squeeze(1)]\n",
    "\n",
    "                # Compute loss\n",
    "                td_errors = target_q_values - values\n",
    "                loss = torch.nn.functional.huber_loss(\n",
    "                    values, target_q_values, reduction=\"none\"\n",
    "                )\n",
    "                loss = (weights * loss).mean()\n",
    "\n",
    "                # Backpropagate and update the network\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_q_values += q_values.mean().item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Update priorities\n",
    "                new_priorities = td_errors.abs().detach().cpu().numpy() + 1e-6\n",
    "                replay_buffer.update_priorities(indices.cpu().numpy(), new_priorities)\n",
    "\n",
    "            # Update target network periodically\n",
    "            if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "                target_net = deepcopy(policy_net)\n",
    "\n",
    "            # Save checkpoints every SAVE_FREQUENCY steps\n",
    "            if total_steps % SAVE_FREQUENCY == 0:\n",
    "                torch.save(\n",
    "                    policy_net.state_dict(),\n",
    "                    f\"../checkpoints/rainbow/checkpoint_{total_steps}.pth\",\n",
    "                )\n",
    "        state = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            # Process remaining transitions in N-step buffer\n",
    "            while len(n_step_buffer) > 0:\n",
    "                n_state, n_action, n_reward, n_next_state, n_done = (\n",
    "                    n_step_buffer[0][0],\n",
    "                    n_step_buffer[0][1],\n",
    "                    0,\n",
    "                    next_state,\n",
    "                    done,\n",
    "                )\n",
    "                for idx, (s, a, r, s_, d) in enumerate(n_step_buffer):\n",
    "                    n_reward += (gamma**idx) * n_step_buffer[idx][2]\n",
    "                    if n_step_buffer[idx][4]:\n",
    "                        n_done = True\n",
    "                        n_next_state = n_step_buffer[idx][3]\n",
    "                        break\n",
    "                replay_buffer.append(\n",
    "                    n_state, n_next_state, np.array([n_action]), n_reward, n_done\n",
    "                )\n",
    "                n_step_buffer.popleft()\n",
    "            break\n",
    "\n",
    "    # Append the total reward for tracking\n",
    "    plot_infos[\"total_reward\"].append(total_reward)\n",
    "    plot_infos[\"epsilon\"].append(epsilon)\n",
    "    plot_infos[\"total_steps\"].append(total_steps)\n",
    "    plot_infos[\"total_q_values\"].append(total_q_values / max(1, episode_steps))\n",
    "    plot_infos[\"total_loss\"].append(total_loss / max(1, episode_steps))\n",
    "\n",
    "    progress_bar.set_description(\n",
    "        f\"R: {plot_infos['total_reward'][-1]}, Îµ: {plot_infos['epsilon'][-1]:.5f}, RSize: {len(replay_buffer)} Q-value: {plot_infos['total_q_values'][-1]:.5f}, Loss: {plot_infos['total_loss'][-1]:.5f}\"\n",
    "    )\n",
    "    progress_bar.update(episode_steps)\n",
    "\n",
    "progress_bar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The trainings were run on Kaggle, so the training logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_plot_infos = pd.DataFrame(plot_infos)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_plot_infos.to_csv(\"../data/rainbow_dqn_plot_infos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "*The tests were run on Kaggle, so the tests logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/3fqdyl697kg4mqs3tkmd7gs80000gn/T/ipykernel_30026/196103572.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
      "/var/folders/mv/3fqdyl697kg4mqs3tkmd7gs80000gn/T/ipykernel_30026/196103572.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1724557170823/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1, Reward: 22.0\n",
      "Game 2, Reward: 13.0\n",
      "Game 3, Reward: 23.0\n",
      "Game 4, Reward: 11.0\n",
      "Game 5, Reward: 15.0\n",
      "Game 6, Reward: 17.0\n",
      "Game 7, Reward: 15.0\n",
      "Game 8, Reward: 23.0\n",
      "Game 9, Reward: 23.0\n",
      "Game 10, Reward: 20.0\n",
      "Game 11, Reward: 15.0\n",
      "Game 12, Reward: 12.0\n",
      "Game 13, Reward: 25.0\n",
      "Game 14, Reward: 22.0\n",
      "Game 15, Reward: 23.0\n",
      "Game 16, Reward: 38.0\n",
      "Game 17, Reward: 30.0\n",
      "Game 18, Reward: 30.0\n",
      "Game 19, Reward: 37.0\n",
      "Game 20, Reward: 21.0\n",
      "Game 21, Reward: 21.0\n",
      "Game 22, Reward: 19.0\n",
      "Game 23, Reward: 12.0\n",
      "Game 24, Reward: 33.0\n",
      "Game 25, Reward: 27.0\n",
      "Game 26, Reward: 11.0\n",
      "Game 27, Reward: 12.0\n",
      "Game 28, Reward: 16.0\n",
      "Game 29, Reward: 22.0\n",
      "Game 30, Reward: 10.0\n",
      "Game 31, Reward: 35.0\n",
      "Game 32, Reward: 16.0\n",
      "Game 33, Reward: 15.0\n",
      "Game 34, Reward: 29.0\n",
      "Game 35, Reward: 25.0\n",
      "Game 36, Reward: 31.0\n",
      "Game 37, Reward: 9.0\n",
      "Game 38, Reward: 19.0\n",
      "Game 39, Reward: 17.0\n",
      "Game 40, Reward: 12.0\n",
      "Game 41, Reward: 23.0\n",
      "Game 42, Reward: 11.0\n",
      "Game 43, Reward: 21.0\n",
      "Game 44, Reward: 23.0\n",
      "Game 45, Reward: 32.0\n",
      "Game 46, Reward: 16.0\n",
      "Game 47, Reward: 19.0\n",
      "Game 48, Reward: 29.0\n",
      "Game 49, Reward: 19.0\n",
      "Game 50, Reward: 15.0\n",
      "Average Reward: 20.68\n",
      "Standard Deviation: 7.406591658786112\n",
      "Max Reward: 38.0\n",
      "Min Reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "# Function to load model weights from checkpoint file\n",
    "def load_checkpoint(model, checkpoint_file):\n",
    "    model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode (important for inference)\n",
    "\n",
    "\n",
    "# Function to play a single episode and return the total reward\n",
    "def play_episode(env, model):\n",
    "    obs, info = env.reset()\n",
    "    state = (\n",
    "        torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # Move to correct device\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < 0.05:\n",
    "            action = env.action_space.sample()  # Random action with 5% probability\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = (\n",
    "                    model(state).argmax(dim=1).item()\n",
    "                )  # Choose action with highest Q-value\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Function to evaluate the model by playing 50 games\n",
    "def evaluate_model(checkpoint_file, num_games=50):\n",
    "    # Create the environment\n",
    "    env = make_env(\"ALE/Breakout-v5\", frame_skip=4)\n",
    "\n",
    "    # Initialize model\n",
    "    action_space = env.action_space.n\n",
    "    model = DuelingDQN(action_space).to(device)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    load_checkpoint(model, checkpoint_file)\n",
    "\n",
    "    total_rewards = []\n",
    "    for game in range(num_games):\n",
    "        total_reward = play_episode(env, model)\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Game {game + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    # Calculate average reward\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    max_reward = np.max(total_rewards)\n",
    "    min_reward = np.min(total_rewards)\n",
    "\n",
    "    print(f\"Average Reward: {avg_reward}\")\n",
    "    print(f\"Standard Deviation: {std_reward}\")\n",
    "    print(f\"Max Reward: {max_reward}\")\n",
    "    print(f\"Min Reward: {min_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model\n",
    "best_checkpoint_path = \"../checkpoints/rainbow/checkpoint_400000.pth\"\n",
    "evaluate_model(best_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
