{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Atari Paper Upgrade using Rainbow\n",
    "Here we will upgrade the DQN Atari paper using the Rainbow algorithm.\n",
    "From the collection of improvements in the Rainbow algorithm, we will implement the following:\n",
    "- Dueling Network Architecture\n",
    "- Prioritized Experience Replay\n",
    "- N-Step Returns\n",
    "- Noisy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025  # Paper used a similar learning rate\n",
    "DISCOUNT_FACTOR = 0.99  # The Î³ discount factor as mentioned in the paper\n",
    "REPLAY_MEMORY_SIZE = 150_000  # Large replay buffer as described, but not too large\n",
    "BATCH_SIZE = 32  # Minibatch size for training\n",
    "TARGET_UPDATE_FREQ = 1_250  # C steps for target network update\n",
    "FRAME_SKIP = 4  # Number of frames skipped\n",
    "MIN_EPSILON = 0.1  # Minimum value of epsilon (for more exploitation)\n",
    "MAX_EPSILON = 1.0  # Starting value of epsilon (for exploration)\n",
    "EPSILON_PHASE = 0.1 # Percentage of steps for epsilon to reach MIN_EPSILON\n",
    "MAX_STEPS = 1_650_000  # Total training episodes\n",
    "REPLAY_START_SIZE = 50_000  # Size of replay memory before starting training\n",
    "SAVE_FREQUENCY = 50_000  # Save model every 50k steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Wrappers\n",
    "We use custom wrappers using gymnasium's API.\n",
    "The paper describes preprocessing the input frames by converting them to grayscale, resizing, normalizing and stacking the last 4 frames to capture motion. We will use the same preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(FrameSkip, self).__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and take the last observation.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        return obs, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "class GrayScaleResize(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Convert the observations to grayscale and resize to 84x84.\"\"\"\n",
    "        super(GrayScaleResize, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)  # Resize to 84x84\n",
    "        return obs\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k=4):\n",
    "        \"\"\"Stack `k` last frames.\"\"\"\n",
    "        super(FrameStack, self).__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(k, shp[0], shp[1]), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_ob(), reward, done, truncated, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)\n",
    "\n",
    "\n",
    "class NormalizeObs(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize observations to the range [0, 1].\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(NormalizeObs, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1.0, shape=env.observation_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay\n",
    "The paper introduces Prioritized Experience Replay to sample important transitions more frequently. We use a SumTree data structure to store priorities and sample transitions based on the priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size, obs_shape, action_shape, alpha=0.6):\n",
    "        \"\"\"Prioritized Experience Replay with sum-tree structure\"\"\"\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((size,), dtype=np.float32)\n",
    "        self.idx = 0\n",
    "\n",
    "    def append(self, t_obs, t1_obs, actions, reward, done, td_error=1.0):\n",
    "        \"\"\"Add a new experience with a given TD-error\"\"\"\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append((t_obs, t1_obs, actions, reward, done))\n",
    "        else:\n",
    "            self.buffer[self.idx] = (t_obs, t1_obs, actions, reward, done)\n",
    "\n",
    "        self.priorities[self.idx] = max_priority\n",
    "        self.idx = (self.idx + 1) % self.size\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"Sample a batch of experiences, with higher probability for higher priority\"\"\"\n",
    "        if len(self.buffer) == self.size:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[: self.idx]\n",
    "\n",
    "        probs = priorities**self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        batch = tuple(\n",
    "            torch.as_tensor(\n",
    "                np.array([sample[field] for sample in samples]), dtype=torch.float32\n",
    "            ).to(device)\n",
    "            for field in range(5)\n",
    "        )\n",
    "        return batch, torch.tensor(weights, dtype=torch.float32).to(device), indices\n",
    "\n",
    "    def update_priorities(self, batch_indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD-error\"\"\"\n",
    "        for idx, error in zip(batch_indices, td_errors):\n",
    "            self.priorities[idx] = abs(error) + 1e-5  # Adding small constant to avoid 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step Returns\n",
    "The paper introduces N-Step Returns to reduce the variance of the Q-value estimates. We use a buffer to store the N-Step Returns and sample transitions from the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepBuffer:\n",
    "    def __init__(self, n=3, gamma=0.99, frame_skip=4):\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        self.frame_skip = frame_skip\n",
    "        self.memory = deque(maxlen=n)\n",
    "\n",
    "    def append(self, experience):\n",
    "        \"\"\"Store experience for N-step returns.\"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def compute_n_step_return(self):\n",
    "        \"\"\"Compute the discounted N-step return over skipped frames.\"\"\"\n",
    "        discounted_reward = 0\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(self.memory):\n",
    "            discounted_reward += (\n",
    "                self.gamma ** (i * self.frame_skip)\n",
    "            ) * reward  # Adjust discount for skipped frames\n",
    "        state, action, _, _, done = self.memory[0]\n",
    "        _, _, _, next_state, _ = self.memory[-1]\n",
    "        return state, action, discounted_reward, next_state, done\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Reset the buffer when starting a new episode.\"\"\"\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Linear Layers\n",
    "The paper introduces Noisy Linear Layers to add noise to the weights of the linear layers. We use a NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.std_init = std_init\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-bound, bound)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-bound, bound)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.weight_epsilon.normal_()\n",
    "            self.bias_epsilon.normal_()\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture\n",
    "Dueling Network Architecture is used in the Rainbow algorithm. The architecture consists of two streams, one for the state value and the other for the advantage values. The two streams are combined to produce the Q-values.\n",
    "\n",
    "We will also use the previously implemented NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDuelingDQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(NoisyDuelingDQN, self).__init__()\n",
    "        # Shared convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Value stream\n",
    "        self.fc1_value = NoisyLinear(3136, 512)\n",
    "        self.value = NoisyLinear(512, 1)\n",
    "\n",
    "        # Advantage stream\n",
    "        self.fc1_advantage = NoisyLinear(3136, 512)\n",
    "        self.advantage = NoisyLinear(512, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flattening\n",
    "\n",
    "        # Compute value stream\n",
    "        value = torch.relu(self.fc1_value(x))\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute advantage stream\n",
    "        advantage = torch.relu(self.fc1_advantage(x))\n",
    "        advantage = self.advantage(advantage)\n",
    "\n",
    "        # Combine value and advantage streams\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network Update\n",
    "Target network helps stabilize learning. The paper mentions that the target network is updated every C steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(policy_net, target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Sync the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the gym environment, by selecting the Breakout game. We specify RMSProp as the optimizer just like in the paper training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, render_mode=None, frame_skip=4):\n",
    "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    env = FrameSkip(env, skip=frame_skip)  # Frame skipping (skip 4 frames)\n",
    "    env = GrayScaleResize(env)  # Convert frames to grayscale and resize to 84x84\n",
    "    env = NormalizeObs(env)  # Normalize pixel values to [0, 1]\n",
    "    env = FrameStack(env, k=4)  # Stack the last 4 frames\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(\"ALE/Breakout-v5\")\n",
    "action_space = [i for i in range(env.action_space.n)]\n",
    "\n",
    "# Initialize Dueling Noisy Networks\n",
    "policy_net = NoisyDuelingDQN(env.action_space.n).to(device)\n",
    "target_net = NoisyDuelingDQN(env.action_space.n).to(device)\n",
    "update_target(policy_net, target_net)\n",
    "\n",
    "# Use Adam instead to speed up training\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = PrioritizedReplayBuffer(REPLAY_MEMORY_SIZE, (4, 84, 84), (1,))\n",
    "n_step_buffer = NStepBuffer(n=3, gamma=DISCOUNT_FACTOR, frame_skip=4)\n",
    "\n",
    "epsilon = MAX_EPSILON  # Starting value of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ['Image Sequence']\n",
    "output_names = ['Q-Values']\n",
    "torch.onnx.export(policy_net.to(\"cpu\"), torch.rand((1, 4, 84, 84)), 'rainbow.onnx', input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "            Conv2d-2             [-1, 64, 9, 9]          32,832\n",
      "            Conv2d-3             [-1, 64, 7, 7]          36,928\n",
      "       NoisyLinear-4                  [-1, 512]               0\n",
      "       NoisyLinear-5                    [-1, 1]               0\n",
      "       NoisyLinear-6                  [-1, 512]               0\n",
      "       NoisyLinear-7                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 77,984\n",
      "Trainable params: 77,984\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.17\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 0.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(policy_net.to(\"cpu\"), input_size=(4, 84, 84), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoisyDuelingDQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1_value): NoisyLinear()\n",
       "  (value): NoisyLinear()\n",
       "  (fc1_advantage): NoisyLinear()\n",
       "  (advantage): NoisyLinear()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we follow the methodology from the paper, by putting all the above components together to train the DQN agent on the Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/500000 [00:00<?, ?it/s]2024-10-01 20:23:20.515 python[91352:2638126] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-01 20:23:20.515 python[91352:2638126] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "R: 4.0, Îµ: 0.99854, RSize: 79 Q-value: 0.00000, Loss: 0.00000:   0%|          | 81/500000 [00:12<21:43:53,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epsilon = (\n",
    "    MAX_EPSILON  # Starting value of epsilon (for exploration-exploitation tradeoff)\n",
    ")\n",
    "beta = 0.4  # Starting beta for importance-sampling correction\n",
    "total_steps = 0\n",
    "plot_infos = {\n",
    "    \"total_steps\": [],\n",
    "    \"total_reward\": [],\n",
    "    \"epsilon\": [],\n",
    "    \"total_q_values\": [],\n",
    "    \"total_loss\": [],\n",
    "}\n",
    "\n",
    "# Progress bar for visualization\n",
    "progress_bar = tqdm(total=MAX_STEPS, desc=\"Training Progress\")\n",
    "\n",
    "while total_steps < MAX_STEPS:\n",
    "    state, info = env.reset()  # Reset environment to initial state\n",
    "    total_reward = 0\n",
    "    total_q_values = 0\n",
    "    total_loss = 0\n",
    "    done = False\n",
    "\n",
    "    # Loop through steps within the episode\n",
    "    while True:\n",
    "        # Use noisy network for action selection (no epsilon-greedy needed)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = (\n",
    "                torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            )\n",
    "            action = policy_net(state_tensor).argmax(dim=1).item()\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Clip rewards between -1 and 1\n",
    "        reward = np.sign(reward)\n",
    "\n",
    "        # Store the transition in the n-step buffer\n",
    "        n_step_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Once n-step buffer is full, calculate n-step return and add to prioritized replay buffer\n",
    "        if len(n_step_buffer.memory) == n_step_buffer.n:\n",
    "            state, action, n_step_reward, next_state, done = (\n",
    "                n_step_buffer.compute_n_step_return()\n",
    "            )\n",
    "            td_error = abs(n_step_reward)  # Initial TD error, used for prioritization\n",
    "            replay_buffer.append(\n",
    "                state, next_state, action, n_step_reward, done, td_error=td_error\n",
    "            )\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        # Only start training when replay memory has enough samples\n",
    "        if len(replay_buffer) >= REPLAY_START_SIZE:\n",
    "            if total_steps % 4 == 0:\n",
    "                # Sample minibatch from prioritized replay buffer\n",
    "                batch, importance_weights, indices = replay_buffer.sample(\n",
    "                    BATCH_SIZE, beta=beta\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    batch_state,\n",
    "                    batch_next_state,\n",
    "                    batch_action,\n",
    "                    batch_reward,\n",
    "                    batch_done,\n",
    "                ) = batch\n",
    "\n",
    "                # Compute current Q-values using the policy network\n",
    "                current_q_values = policy_net(batch_state).gather(\n",
    "                    1, batch_action.unsqueeze(1).long()\n",
    "                )\n",
    "\n",
    "                # Compute target Q-values using the target network (for the next state)\n",
    "                next_q_values = target_net(batch_next_state).max(1)[0].detach()\n",
    "\n",
    "                # Calculate TD target with N-step rewards\n",
    "                target_q_values = batch_reward + (\n",
    "                    DISCOUNT_FACTOR * next_q_values * torch.logical_not(batch_done)\n",
    "                )\n",
    "\n",
    "                # Compute loss with importance-sampling weights\n",
    "                loss = (\n",
    "                    importance_weights\n",
    "                    * (current_q_values.squeeze() - target_q_values).pow(2)\n",
    "                ).mean()\n",
    "\n",
    "                total_q_values += target_q_values.mean().item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backpropagation and update the network\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update priorities in the buffer based on new TD-errors\n",
    "                new_td_errors = (\n",
    "                    (current_q_values.squeeze() - target_q_values)\n",
    "                    .abs()\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "                replay_buffer.update_priorities(indices, new_td_errors)\n",
    "\n",
    "            # Update target network periodically\n",
    "            if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "                update_target(policy_net, target_net)\n",
    "\n",
    "            # Save checkpoints every SAVE_FREQUENCY steps\n",
    "            if total_steps % SAVE_FREQUENCY == 0:\n",
    "                torch.save(\n",
    "                    policy_net.state_dict(),\n",
    "                    f\"checkpoints/upgraded/checkpoint_{total_steps}.pth\",\n",
    "                )\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    n_step_buffer.clear()  # Clear the n-step buffer after each episode\n",
    "\n",
    "    # Decay beta and epsilon over time\n",
    "    beta = min(1.0, beta + (1.0 - 0.4) / MAX_STEPS)\n",
    "    epsilon = max(MIN_EPSILON, epsilon - (MAX_EPSILON - MIN_EPSILON) / (EPSILON_PHASE * MAX_STEPS))\n",
    "\n",
    "    # Log training progress\n",
    "    plot_infos[\"total_reward\"].append(total_reward)\n",
    "    plot_infos[\"epsilon\"].append(epsilon)\n",
    "    plot_infos[\"total_steps\"].append(total_steps)\n",
    "    plot_infos[\"total_q_values\"].append(total_q_values / total_steps)\n",
    "    plot_infos[\"total_loss\"].append(total_loss / total_steps)\n",
    "\n",
    "    progress_bar.set_description(\n",
    "        f\"R: {plot_infos['total_reward'][-1]}, Îµ: {plot_infos['epsilon'][-1]:.5f}, RSize: {len(replay_buffer)} Q-value: {plot_infos['total_q_values'][-1]:.5f}, Loss: {plot_infos['total_loss'][-1]:.5f}\"\n",
    "    )\n",
    "    progress_bar.update(plot_infos[\"total_steps\"][-1] - progress_bar.n)\n",
    "\n",
    "progress_bar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training progress of the DQN agent on the Breakout game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_infos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplot_dict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_dict\n\u001b[0;32m----> 3\u001b[0m plot_dict(\u001b[43mplot_infos\u001b[49m, REPLAY_START_SIZE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_infos' is not defined"
     ]
    }
   ],
   "source": [
    "from ..utils.plot_dict import plot_dict\n",
    "\n",
    "plot_dict(plot_infos, REPLAY_START_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R: 0.0, Îµ: 0.95102, RSize: 2719 Q-value: 0.00000, Loss: 0.00000:   1%|          | 2721/500000 [00:20<30:17, 273.61it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_plot_infos = pd.DataFrame(plot_infos)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_plot_infos.to_csv('data/rainbow_dqn_plot_infos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "Here is the training playing each 50000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/3fqdyl697kg4mqs3tkmd7gs80000gn/T/ipykernel_2397/1956220756.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
      "2024-10-02 00:08:39.117 python[2397:2838176] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-02 00:08:39.117 python[2397:2838176] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1, Reward: 22.0\n",
      "Game 2, Reward: 19.0\n",
      "Game 3, Reward: 19.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to load model weights from checkpoint file\n",
    "def load_checkpoint(model, checkpoint_file):\n",
    "    model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode (important for inference)\n",
    "\n",
    "\n",
    "# Function to play a single episode and return the total reward\n",
    "def play_episode(env, model):\n",
    "    obs, info = env.reset()\n",
    "    state = (\n",
    "        torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # Move to correct device\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < 0.05:\n",
    "            action = env.action_space.sample()  # Random action with 5% probability\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(state).argmax(dim=1).item()  # Choose action with highest Q-value\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Function to evaluate the model by playing 50 games\n",
    "def evaluate_model(checkpoint_file, num_games=50):\n",
    "    # Create the environment\n",
    "    env = make_env(\"ALE/Breakout-v5\")\n",
    "\n",
    "    # Initialize model\n",
    "    action_space = env.action_space.n\n",
    "    model = NoisyDuelingDQN(action_space).to(device)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    load_checkpoint(model, checkpoint_file)\n",
    "\n",
    "    total_rewards = []\n",
    "    for game in range(num_games):\n",
    "        total_reward = play_episode(env, model)\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Game {game + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    # Calculate average reward\n",
    "    avg_reward = sum(total_rewards) / num_games\n",
    "    print(f\"Average reward over {num_games} games: {avg_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model\n",
    "best_checkpoint_path = (\n",
    "    \"../checkpoints/upgraded/checkpoint_500000.pth\"  # Replace with actual path\n",
    ")\n",
    "evaluate_model(best_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
