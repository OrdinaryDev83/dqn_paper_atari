{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZVlzTE4fU6D"
      },
      "source": [
        "# DQN Atari Paper Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibtHQ-GLQ0ZB",
        "outputId": "9bad0992-1656-4bd9-bdf9-8024ed2874b7"
      },
      "outputs": [],
      "source": [
        "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aGO3B2XIQGPQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQK2jVLaLQ7G",
        "outputId": "318589d9-4d96-4655-c5cc-be6f027ebbd0"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0002  # Paper used a similar learning rate\n",
        "DISCOUNT_FACTOR = 0.99  # The Î³ discount factor as mentioned in the paper\n",
        "REPLAY_MEMORY_SIZE = 150_000  # Large replay buffer as described, but not too large\n",
        "MINI_BATCH_SIZE = 32  # Minibatch size for training\n",
        "TARGET_UPDATE_FREQ = 1_200  # C steps for target network update\n",
        "FRAME_SKIP = 4  # Number of frames skipped\n",
        "MIN_EPSILON = 0.1  # Minimum value of epsilon (for more exploitation)\n",
        "MAX_EPSILON = 1.0  # Starting value of epsilon (for exploration)\n",
        "EPSILON_PHASE = 0.1  # Percentage of steps for epsilon to reach MIN_EPSILON\n",
        "MAX_STEPS = 2_500_000  # Total training episodes\n",
        "REPLAY_START_SIZE = 75_000  # Size of replay memory before starting training\n",
        "SAVE_FREQUENCY = 500_000  # Save model every 50k steps\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gymnasium Environment Setup\n",
        "Here we set up the gym environment, by selecting the Breakout game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jWqrjB-NAAp8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "[Powered by Stella]\n"
          ]
        }
      ],
      "source": [
        "def make_env(env_id, render_mode=None, frame_skip=4):\n",
        "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
        "    env = gym.make(env_id, render_mode=render_mode, frameskip=1)\n",
        "    # Handles resizing, grayscale, frameskip and stacking of frames\n",
        "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
        "    # Record statistics like the precise score and lives\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    # Stack 4 frames\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env = gym.wrappers.AutoResetWrapper(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "env = make_env(\"ALE/Breakout-v5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target Network Update\n",
        "Target network helps stabilize learning. The paper mentions that the target network is updated every 1,250 steps.\n",
        "Unlike the paper here we will use a Double DQN approach where the target network is updated with the weights of the main network every C steps. This helps in reducing overestimation of Q-values and training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Q-Network Architecture\n",
        "Referring to the paper's architecture (3 convolutional layers, followed by fully connected layers). We also normalize the input states to the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DPZFfc5QX5t",
        "outputId": "8fe3c0b6-9223-4c0f-f55d-d37a29f04c42"
      },
      "outputs": [],
      "source": [
        "class DeepQNetwork(torch.nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.conv = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.linear = torch.nn.Sequential(\n",
        "            torch.nn.Linear(7 * 7 * 64, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x / 255.0)\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buidQaDQf0tZ",
        "outputId": "e2fe19b3-ebdb-42bc-a7e6-a8de0dde96bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 1,686,180\n"
          ]
        }
      ],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params\n",
        "\n",
        "\n",
        "print(\n",
        "    f\"Trainable parameters: {count_trainable_parameters(DeepQNetwork(env.action_space.n)):,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Ty-fh84Gk4"
      },
      "source": [
        "## Replay Memory\n",
        "We implement an experience replay buffer as described in the paper to store past experiences and sample them randomly during training to break the correlation between consecutive frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ii70AQ_nft0k"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size, obs_shape, action_shape):\n",
        "        \"\"\"\n",
        "        Initialize the replay buffer\n",
        "\n",
        "        Args:\n",
        "            size : int  Size of the replay buffer\n",
        "            obs_shape :ctuple  Shape of the observations\n",
        "            action : tuple  Shape of the actions\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.obs_shape = obs_shape\n",
        "        self.action_shape = action_shape\n",
        "\n",
        "        self.t_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
        "        self.t1_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
        "        self.actions = np.empty((size, *action_shape), dtype=np.uint8)\n",
        "        self.rewards = np.empty(size, dtype=np.float16)\n",
        "        self.dones = np.empty(size, dtype=np.bool_)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.current_size = 0\n",
        "\n",
        "    def append(self, t_obs, t1_obs, actions, reward, done):\n",
        "        \"\"\"\n",
        "        Append a new transition to the replay buffer\n",
        "\n",
        "        Args:\n",
        "            t_obs : np.array  Current observation\n",
        "            t1_obs : np.array  Next observation\n",
        "            actions : np.array  Action\n",
        "            reward : float  Reward\n",
        "            done : bool  Done\n",
        "        \"\"\"\n",
        "        self.t_obs[self.idx] = t_obs\n",
        "        self.t1_obs[self.idx] = t1_obs\n",
        "        self.actions[self.idx] = actions\n",
        "        self.rewards[self.idx] = reward\n",
        "        self.dones[self.idx] = done\n",
        "\n",
        "        self.current_size = min(self.current_size + 1, self.size)\n",
        "        self.idx = (self.idx + 1) % self.size\n",
        "\n",
        "    def get_minibatch(self, batch_size, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Sample a minibatch from the replay buffer\n",
        "\n",
        "        Args:\n",
        "            batch_size : int  Size of the minibatch\n",
        "            device : str  Device to use\n",
        "        \"\"\"\n",
        "        ids = np.random.choice(self.current_size, batch_size, replace=False)\n",
        "        batch = (\n",
        "            self.t_obs[ids],\n",
        "            self.t1_obs[ids],\n",
        "            self.actions[ids],\n",
        "            self.rewards[ids],\n",
        "            self.dones[ids],\n",
        "        )\n",
        "\n",
        "        return tuple(\n",
        "            torch.as_tensor(item, dtype=torch.float32).to(device) for item in batch\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB3RBqeqfwTz"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqHe7Fma4hXS"
      },
      "source": [
        "The main constraint is that the training is relatively long and requires a lot of time. The limit is that Colab automatically stops after a certain time. The second limitation is that it is difficult to reproduce the results of the paper because we do not have enough memory to reach a replay buffer of 1 million.\n",
        "\n",
        "The implementation is as follows:\n",
        "\n",
        "1. We start by defining two Q models: one is updated more regularly and the other is the target network, which is a copy of the Q model at a given time t. We use the Adam optimizer, which seems to offer better results with my hyperparameters.\n",
        "2. T corresponds to the total number of timesteps and not the number of timesteps in an episode. We can proceed this way because AutoResetWrapper takes care of resetting the environment when we have 0 lives.\n",
        "3. We set up an exploratory policy, reducing linearly over 10% of the timesteps. We start with an exploration of 1.0, then linearly decrease to 0.1.\n",
        "4. Initially, we fill the replay buffer with 75,000 elements before starting the training.\n",
        "5. As described in the paper, we choose to limit the reward based on its sign.\n",
        "6. We also perform a checkpoint every 500,000 steps.\n",
        "7. Finally, every 1,250 steps, we update the target network.\n",
        "The implementation is as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tWaBdGONQgOo",
        "outputId": "d481effd-6418-4c1a-b4dc-90970f2d7114"
      },
      "outputs": [],
      "source": [
        "n_actions = env.action_space.n\n",
        "dqn = DeepQNetwork(n_actions).to(device)\n",
        "optimizer = torch.optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
        "dqn_prime = DeepQNetwork(n_actions).to(device)\n",
        "\n",
        "buffer = ReplayBuffer(REPLAY_MEMORY_SIZE, (4, 84, 84), (1,))\n",
        "training_history = {\"loss\": [0], \"mean_q_value\": [0], \"episode_rewards\": [0], \"steps\": [0]}\n",
        "\n",
        "t_observation, _ = env.reset()\n",
        "episode_reward = 0\n",
        "\n",
        "progress_bar = tqdm(range(MAX_STEPS), desc=\"Training Progress\")\n",
        "\n",
        "episode_steps = 0\n",
        "episode_loss = 0\n",
        "episode_q_values = 0\n",
        "\n",
        "for t in progress_bar:\n",
        "    # Epsilon with linear decay\n",
        "    eps = max(\n",
        "        MIN_EPSILON,\n",
        "        MIN_EPSILON\n",
        "        + (MAX_EPSILON - MIN_EPSILON) * (1 - t / (EPSILON_PHASE * MAX_STEPS)),\n",
        "    )\n",
        "\n",
        "    # epsilon-greedy policy\n",
        "    if np.random.rand(1) < eps:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = dqn(\n",
        "                torch.tensor(np.array(t_observation), device=device).unsqueeze(0)\n",
        "            )\n",
        "            action = torch.argmax(q_values, dim=1).item()\n",
        "            episode_q_values += q_values.mean().item()\n",
        "\n",
        "    # store the action in the replay buffer\n",
        "    t1_observation, reward, done, _, info = env.step(action)\n",
        "    buffer.append(\n",
        "        t_observation, t1_observation, np.array([action]), np.sign(reward), done\n",
        "    )\n",
        "    episode_reward += reward\n",
        "\n",
        "    # appears when the episode is done\n",
        "    if \"final_info\" in info:\n",
        "        training_history[\"steps\"].append(t)\n",
        "        training_history[\"episode_rewards\"].append(episode_reward)\n",
        "        training_history[\"mean_q_value\"].append(episode_q_values / episode_steps)\n",
        "        training_history[\"loss\"].append(episode_loss / episode_steps)\n",
        "        episode_reward = 0\n",
        "        progress_bar.set_description(\n",
        "            f\"R: {training_history['episode_rewards'][-1]:.2f}, l: {training_history['loss'][-1]:.2f}, Mean Q: {training_history['mean_q_value'][-1]:.2f}, e: {eps:.2f}\"\n",
        "        )\n",
        "        episode_steps = 0\n",
        "        episode_loss = 0\n",
        "        episode_q_values = 0\n",
        "\n",
        "    # Check point every 500_000 step\n",
        "    if t > 0 and t % SAVE_FREQUENCY == 0:\n",
        "        torch.save(dqn.state_dict(), f\"checkpoint{t}.pt\")\n",
        "\n",
        "    if t > REPLAY_START_SIZE:\n",
        "        if t % 4 == 0:\n",
        "            # Sample a minibatch\n",
        "            t_obs, t1_obs, actions, rewards, dones = buffer.get_minibatch(\n",
        "                MINI_BATCH_SIZE, device=device\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                not_done = ~dones.bool()\n",
        "                a_prime = dqn_prime(t1_obs).amax(dim=1)\n",
        "                y_j = rewards + DISCOUNT_FACTOR * a_prime * not_done\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform a gradient descent step on\n",
        "            q_values = dqn(t_obs)\n",
        "            idx = torch.arange(actions.size(0)).to(device).long()\n",
        "            values = q_values[idx, actions.squeeze().long()]\n",
        "            \n",
        "            loss = torch.nn.functional.huber_loss(y_j, values)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            episode_loss += loss.item()\n",
        "            \n",
        "        if t % TARGET_UPDATE_FREQ == 0:\n",
        "            dqn_prime = deepcopy(dqn)\n",
        "        \n",
        "    episode_steps += 1\n",
        "\n",
        "    t_observation = t1_observation\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The trainings were run on Kaggle, so the training logs are not included in this notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the data for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_plot_infos = pd.DataFrame(training_history)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_plot_infos.to_csv('../data/original_training_history.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmReXz8GxrtF"
      },
      "source": [
        "## Result\n",
        "*The tests were run on Kaggle, so the tests logs are not included in this notebook.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mv0071RSuegy"
      },
      "outputs": [],
      "source": [
        "def test_model(model_path):\n",
        "    \"\"\"\n",
        "    Test the model\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the model checkpoint to load\n",
        "    \"\"\"\n",
        "\n",
        "    env = gym.make(\"ALE/Breakout-v5\", frameskip=1, render_mode=\"rgb_array\")\n",
        "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env = gym.wrappers.AutoResetWrapper(env)\n",
        "\n",
        "    # load model\n",
        "    dqn = DeepQNetwork(env.action_space.n).to(device)\n",
        "    state_dict = torch.load(model_path, map_location=torch.device(device))\n",
        "    dqn.load_state_dict(state_dict=state_dict)\n",
        "    dqn.eval()\n",
        "\n",
        "    t_observation, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    info = None\n",
        "    while not done:\n",
        "        if np.random.rand(1) < 0.01:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            q_values = dqn(\n",
        "            torch.tensor(np.array(t_observation), device=device).unsqueeze(0)\n",
        "            )\n",
        "            action = torch.argmax(q_values, dim=1).cpu().numpy().squeeze()\n",
        "        t_observation, reward, done, _, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "\n",
        "    print(f\"Total reward: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_n1dVN56NgA"
      },
      "outputs": [],
      "source": [
        "rewards = []\n",
        "for i in tqdm(range(50)):\n",
        "    rewards.append(test_model(f\"../checkpoints/original/checkpoint1500000.pt\"))\n",
        "\n",
        "print(f\"Mean reward: {np.mean(rewards)}\")\n",
        "print(f\"Std reward: {np.std(rewards)}\")\n",
        "print(f\"Max reward: {np.max(rewards)}\")\n",
        "print(f\"Min reward: {np.min(rewards)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
